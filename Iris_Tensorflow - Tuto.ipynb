{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer les librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des données\n",
    "\n",
    "Dans un premier temps, nous devons récupérer notre base de données qui vas nous servir d'entrainement. Ces données sont fournis par Tensorflow. Elles représentent 120 donnée de 3 fleurs différentes sur 5 colonnes. Les colonnes sont: \n",
    "longueur pétale - largeur pétale - longueur sépale - largeur sépale - type de fleur(0, 1 ou 2)\n",
    "\n",
    "Après avoir affiché ces données, faites une fonction qui crée deux arrays, features qui contiendra les données des fleurs, et labels qui correspondra aux type de fleurs que l'on voudra prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    print(\"La taille des données est \",Train_Iris_Data.shape)\n",
    "    data=np.array(data)\n",
    "    \n",
    "    ### Votre code\n",
    "\n",
    "    return features,labels\n",
    "\n",
    "#Download the csv\n",
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "Train_Iris_Data = pd.read_csv(train_dataset_url)\n",
    "\n",
    "features, labels = split_data(Train_Iris_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour observer un exemple de donnée.\n",
    "print(\"example features:\", features[0])\n",
    "print(\"example label:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabriquer le modèle d'entrainement\n",
    "\n",
    "Tout d'abbord, nous allons créer les placeholders x et y, deux variables qui voons nous servir à insérer nos données dans le modèle plus tard:\n",
    "https://www.tensorflow.org/api_docs/python/tf/placeholder\n",
    "\n",
    "x recevant un tableau de features et y un tableau de labels, il faut bien choisir la forme des placeholders en fonction des entrées et des sorties du réseau de neurone.\n",
    "\n",
    "Attention, le nombre de ligne des placeholders sont variable car on peut leurs donner des tableaux de différentes longueurs, écrivez \"None\" à la place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser les Placeholders\n",
    "x = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, nous allons créer le réseaux de neuronne. Nous allons utiliser deux couches différentes dont les neurones sont tous connecté les un aux autres. Nous aurons donc deux couches avec une fonction d'activation ReLU, et une avec une fonction Softmax. Nous allons ici les coder nous même afin de comprendre comment cela fonctionne, il existe néanmoins des API plus au niveaux qui font cela à notre place.\n",
    "\n",
    "Du coup nous allons créer deux fonctions qui vont créer une couche de neurone chacunes.\n",
    "\n",
    "Pour chaques fonctions, créez les matrices de poid et de biais à partir de tf.Variable().\n",
    "Pour les poids, initialisez les suivant une loi normal avec tf.truncated_normal avec une moyenne à 0 et une déviaion à 0.1.\n",
    "https://www.tensorflow.org/api_docs/python/tf/truncated_normal\n",
    "Pour les biais, vous pouvez les initialiser à 0 avec https://www.tensorflow.org/api_docs/python/tf/zeros\n",
    "\n",
    "\n",
    "Puis pour chaque fonction appliquez la fonction d'activation https://www.tensorflow.org/api_docs/python/tf/nn/relu et https://www.tensorflow.org/api_docs/python/tf/nn/softmax sur la somme des entrées de la couche multiplié par les poids plus les biais soit f((x*poid)+biais).\n",
    "\n",
    "Attention, pour la multiplication, utilise tf.matmul()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dense_relu_layer(x, num_neurons_previous_layer, num_neurons_current_layer, name_scope=\"relu\"):\n",
    "    with tf.name_scope(name_scope):\n",
    "        # Model parameters\n",
    "        weights =\n",
    "        biases = \n",
    "        # relu layer\n",
    "        \n",
    "        \n",
    "        result = \n",
    "        return result\n",
    "    \n",
    "def add_dense_softmax_layer(x, num_neurons_previous_layer, num_sortie, name_scope=\"softmax\"):\n",
    "    with tf.name_scope(name_scope):\n",
    "        # Model parameters\n",
    "        weights = \n",
    "        biases = \n",
    "        # softmax layer\n",
    "        \n",
    "        \n",
    "        result =\n",
    "        return result\n",
    "\n",
    "# Creation of the Neural network\n",
    "dense = add_dense_relu_layer(x=x, num_neurons_previous_layer=4, num_neurons_current_layer=10, name_scope=\"relu\")\n",
    "dense2 = add_dense_relu_layer(x=dense, num_neurons_previous_layer=10, num_neurons_current_layer=10, name_scope=\"relu2\")\n",
    "softmax = add_softmax_op(x=dense2, num_neurons_previous_layer=10, num_classes=3, name_scope=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, pour finir le modèle, on vas définir la fonction de coût qui calcule la différence entre la sortie du réseau de neurone et les labels qu'on lui donne durant l'entrainement. Pour cela, nous allons utiliser la fonction de cross entropy: https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "\n",
    "Ici, train_op est l'opérateur qui entraine et modifie les poids du réseau de neurone. Il utiliser l'algorithme de descente de gradient afin de trouver les poids et les biais qui minimisent la fonction de cout.\n",
    "\n",
    "Ensuite, on définit notre probabilité de prédiction. Utilisez tf.argmax pour trouver ce que prédit le réseau de neurone, puis tf.equals pour compter le nombre de bonne prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    cross_entropy = \n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "with tf.name_scope(\"Train\"):\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate= 0.02).minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # Convert logits to label indexes\n",
    "    prediction = \n",
    "    num_correct_prediction = \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement\n",
    "\n",
    "Maintenant que nous avons définis notre modèle de réseau de neurone et tous nos opérateur, nous pouvons passer à l'étape d'entrainement. Le but sera d'appeler la fonction train_op en fournissant au réseau les features et les labels afin que sa sortie correspondent aux labels. On veut également afficher la perte et le taux de prédiction pour chaque boucle, donnés par les opérateurs loss et accuracy pour voir leurs évolutions.\n",
    "\n",
    "On initialise une session dans un premier temps. Pour appeler les opérateurs du modèle, il faut utiliser Session.run(). Pour pouvoir entrainer le réseau, il faut pouvoir lui donner les données d'entrainements et les lables. Pour ce faire, on utilise feed_dict{} où l'on vas fournir aux placeholders définis précedemment les valeurs d'entrainements.\n",
    "Le commande serait de la forme: \n",
    "            \n",
    "            Session.run([opérateur1, opérateur2], feed_dict={placeholder1:i, placeholder2:y})\n",
    "            \n",
    "Puisque l'on fait une boucle d'entrainement de 10 000 tours, l'entrainement peut prendre quelques secondes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "# Initilize the Session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize the model's variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    ### Votre code\n",
    "    loss_val, accuracy_val =  ### Votre Code\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\" Epoch :\", i,\" Loss: \", loss_val,\" Accuracy: \", accuracy_val,\"Accuracy Test: \",test_accuracy_val)\n",
    "\n",
    "    train_loss_results.append(loss_val)\n",
    "    train_accuracy_results.append(accuracy_val)\n",
    "            \n",
    "# Plot the evolution of loss and train accuracy\n",
    "fig, axes = plt.subplots(2)\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Après l'entrainement, on veut observer l'efficacité du réseau de neurone. On va donc calculer le taux de prédiction pour une base de donnée test, différente de celle d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "Test_Iris_Data = pd.read_csv(test_url)\n",
    "\n",
    "features_test, labels_test = split_data(Train_Iris_Data)\n",
    "\n",
    "test_accuracy_val = ###Votre Code\n",
    "\n",
    "print (\"Test accuracy:\", test_accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Enfin, nous pouvons utiliser le réseau pour faire nos prédictions avec des données dont nous ne connaissons pas la sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"]\n",
    "\n",
    "features_pred = np.array ([    \n",
    "    [5.1, 3.3, 1.7, 0.5,],\n",
    "    [5.9, 3.0, 4.2, 1.5,],\n",
    "    [6.9, 3.1, 5.4, 2.1]])\n",
    "predictions = ### Votre Code\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
